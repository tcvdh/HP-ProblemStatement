@misc{jiang2024megascalescalinglargelanguage,
    title = {MegaScale: Scaling Large Language Model Training to More Than 10,
             000 GPUs},
    author = {Ziheng Jiang and Haibin Lin and Yinmin Zhong and Qi Huang and
              Yangrui Chen and Zhi Zhang and Yanghua Peng and Xiang Li and Cong
              Xie and Shibiao Nong and Yulu Jia and Sun He and Hongmin Chen and
              Zhihao Bai and Qi Hou and Shipeng Yan and Ding Zhou and Yiyao Sheng
              and Zhuo Jiang and Haohan Xu and Haoran Wei and Zhang Zhang and
              Pengfei Nie and Leqi Zou and Sida Zhao and Liang Xiang and Zherui
              Liu and Zhe Li and Xiaoying Jia and Jianxi Ye and Xin Jin and Xin
              Liu},
    year = {2024},
    eprint = {2402.15627},
    archivePrefix = {arXiv},
    primaryClass = {cs.LG},
    url = {https://arxiv.org/abs/2402.15627},
}
@misc{luccioni2022estimatingcarbonfootprintbloom,
    title = {Estimating the Carbon Footprint of BLOOM, a 176B Parameter Language
             Model},
    author = {Alexandra Sasha Luccioni and Sylvain Viguier and Anne-Laure
              Ligozat},
    year = {2022},
    eprint = {2211.02001},
    archivePrefix = {arXiv},
    primaryClass = {cs.LG},
    url = {https://arxiv.org/abs/2211.02001},
}
@misc{narayanan2021efficientlargescalelanguagemodel,
    title = {Efficient Large-Scale Language Model Training on GPU Clusters Using
             Megatron-LM},
    author = {Deepak Narayanan and Mohammad Shoeybi and Jared Casper and Patrick
              LeGresley and Mostofa Patwary and Vijay Anand Korthikanti and
              Dmitri Vainbrand and Prethvi Kashinkunti and Julie Bernauer and
              Bryan Catanzaro and Amar Phanishayee and Matei Zaharia},
    year = {2021},
    eprint = {2104.04473},
    archivePrefix = {arXiv},
    primaryClass = {cs.CL},
    url = {https://arxiv.org/abs/2104.04473},
}

@article{Ji2026-gp,
    title = {A systematic review of electricity demand for large language
             models: evaluations, challenges, and solutions},
    volume = {225},
    ISSN = {1364-0321},
    url = {http://dx.doi.org/10.1016/j.rser.2025.116159},
    DOI = {10.1016/j.rser.2025.116159},
    journal = {Renewable and Sustainable Energy Reviews},
    publisher = {Elsevier BV},
    author = {Ji, Zhenya and Jiang, Ming},
    year = {2026},
    month = jan,
    pages = {116159},
}

@misc{lin2025understandingstragglerslargemodel,
    title = {Understanding Stragglers in Large Model Training Using What-if
             Analysis},
    author = {Jinkun Lin and Ziheng Jiang and Zuquan Song and Sida Zhao and
              Menghan Yu and Zhanghan Wang and Chenyuan Wang and Zuocheng Shi and
              Xiang Shi and Wei Jia and Zherui Liu and Shuguang Wang and Haibin
              Lin and Xin Liu and Aurojit Panda and Jinyang Li},
    year = {2025},
    eprint = {2505.05713},
    archivePrefix = {arXiv},
    primaryClass = {cs.DC},
    url = {https://arxiv.org/abs/2505.05713},
}

@misc{patterson2021carbonemissionslargeneural,
    title = {Carbon Emissions and Large Neural Network Training},
    author = {David Patterson and Joseph Gonzalez and Quoc Le and Chen Liang and
              Lluis-Miquel Munguia and Daniel Rothchild and David So and Maud
              Texier and Jeff Dean},
    year = {2021},
    eprint = {2104.10350},
    archivePrefix = {arXiv},
    primaryClass = {cs.LG},
    url = {https://arxiv.org/abs/2104.10350},
}

@inproceedings{xu2024efficient,
    author = {Xu, Wubiao and Huang, Xin and Meng, Shiman and Zhang, Weiping and
              Guo, Luanzheng and Sato, Kento},
    title = {An Efficient Checkpointing System for Large Machine Learning Model
             Training},
    booktitle = {Workshops of the International Conference on High Performance
                 Computing, Network, Storage, and Analysis (SC-W)},
    year = {2024},
    month = {11},
    pages = {896--900},
    doi = {10.1109/SCW63240.2024.00127},
}

@misc{zhang2025gockptgradientassistedmultistepoverlapped,
    title = {GoCkpt: Gradient-Assisted Multi-Step overlapped Checkpointing for
             Efficient LLM Training},
    author = {Keyao Zhang and Yiquan Chen and Zhuo Hu and Wenhai Lin and
              Jiexiong Xu and Wenzhi Chen},
    year = {2025},
    eprint = {2511.07035},
    archivePrefix = {arXiv},
    primaryClass = {cs.OS},
    url = {https://arxiv.org/abs/2511.07035},
}

@misc{kaplan2020scalinglawsneurallanguage,
    title = {Scaling Laws for Neural Language Models},
    author = {Jared Kaplan and Sam McCandlish and Tom Henighan and Tom B. Brown
              and Benjamin Chess and Rewon Child and Scott Gray and Alec Radford
              and Jeffrey Wu and Dario Amodei},
    year = {2020},
    eprint = {2001.08361},
    archivePrefix = {arXiv},
    primaryClass = {cs.LG},
    url = {https://arxiv.org/abs/2001.08361},
}

@inproceedings{9499454,
    author = {Mastenbroek, Fabian and Andreadis, Georgios and Jounaid, Soufiane
              and Lai, Wenchen and Burley, Jacob and Bosch, Jaro and van Eyk,
              Erwin and Versluis, Laurens and van Beek, Vincent and Iosup,
              Alexandru},
    booktitle = {2021 IEEE/ACM 21st International Symposium on Cluster, Cloud
                 and Internet Computing (CCGrid)},
    title = {OpenDC 2.0: Convenient Modeling and Simulation of Emerging
             Technologies in Cloud Datacenters},
    year = {2021},
    volume = {},
    number = {},
    pages = {455-464},
    keywords = {Procurement;Cloud computing;Analytical models;Computational
                modeling;Machine learning;Reproducibility of
                results;Stakeholders;OpenDC;datacenter;simulation;modeling;usecases;experimentation;performance
                analysis},
    doi = {10.1109/CCGrid51090.2021.00055},
}

@article{dryden2025survey,
    title = {A Survey of End-to-End Modeling for Distributed DNN Training:
             Workloads, Simulators, and TCO},
    author = {Dryden, E. and others},
    journal = {arXiv preprint arXiv:2506.09275},
    year = {2025},
}

@article{lin2025stragglers,
    title = {Understanding Stragglers in Large Model Training Using What-if
             Analysis},
    author = {Lin, Jinkun and Jiang, Ziheng and Song, Zuquan and Zhao, Sida and
              Yu, Menghan and Wang, Zhanghan and Wang, Chenyuan and Shi, Zuocheng
              and Shi, Xiang and Jia, Wei and others},
    journal = {arXiv preprint arXiv:2505.05713},
    year = {2025},
}


@article{10.5120/ijca2025925323,
    author = { Krishnam Raju Narsepalle },
    title = { Energy-Efficient Training and Inference in Large Language Models:
             Optimizing Computational and Energy Costs },
    journal = { International Journal of Computer Applications },
    issue_date = { Jun 2025 },
    volume = { 187 },
    number = { 14 },
    month = { Jun },
    year = { 2025 },
    issn = { 0975-8887 },
    pages = { 1-13 },
    numpages = {9},
    url = {
           https://ijcaonline.org/archives/volume187/number14/energy-efficient-training-and-inference-in-large-language-models-optimizing-computational-and-energy-costs/
           },
    doi = { 10.5120/ijca2025925323 },
    publisher = {Foundation of Computer Science (FCS), NY, USA},
    address = {New York, USA},
}

