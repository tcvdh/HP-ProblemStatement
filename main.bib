@misc{jiang2024megascalescalinglargelanguage,
    title = {MegaScale: Scaling Large Language Model Training to More Than 10,
             000 GPUs},
    author = {Ziheng Jiang and Haibin Lin and Yinmin Zhong and Qi Huang and
              Yangrui Chen and Zhi Zhang and Yanghua Peng and Xiang Li and Cong
              Xie and Shibiao Nong and Yulu Jia and Sun He and Hongmin Chen and
              Zhihao Bai and Qi Hou and Shipeng Yan and Ding Zhou and Yiyao Sheng
              and Zhuo Jiang and Haohan Xu and Haoran Wei and Zhang Zhang and
              Pengfei Nie and Leqi Zou and Sida Zhao and Liang Xiang and Zherui
              Liu and Zhe Li and Xiaoying Jia and Jianxi Ye and Xin Jin and Xin
              Liu},
    year = {2024},
    eprint = {2402.15627},
    archivePrefix = {arXiv},
    primaryClass = {cs.LG},
    url = {https://arxiv.org/abs/2402.15627},
}
@misc{luccioni2022estimatingcarbonfootprintbloom,
    title = {Estimating the Carbon Footprint of BLOOM, a 176B Parameter Language
             Model},
    author = {Alexandra Sasha Luccioni and Sylvain Viguier and Anne-Laure
              Ligozat},
    year = {2022},
    eprint = {2211.02001},
    archivePrefix = {arXiv},
    primaryClass = {cs.LG},
    url = {https://arxiv.org/abs/2211.02001},
}
@misc{narayanan2021efficientlargescalelanguagemodel,
    title = {Efficient Large-Scale Language Model Training on GPU Clusters Using
             Megatron-LM},
    author = {Deepak Narayanan and Mohammad Shoeybi and Jared Casper and Patrick
              LeGresley and Mostofa Patwary and Vijay Anand Korthikanti and
              Dmitri Vainbrand and Prethvi Kashinkunti and Julie Bernauer and
              Bryan Catanzaro and Amar Phanishayee and Matei Zaharia},
    year = {2021},
    eprint = {2104.04473},
    archivePrefix = {arXiv},
    primaryClass = {cs.CL},
    url = {https://arxiv.org/abs/2104.04473},
}

@article{Ji2026-gp,
    title = {A systematic review of electricity demand for large language
             models: evaluations, challenges, and solutions},
    volume = {225},
    ISSN = {1364-0321},
    url = {http://dx.doi.org/10.1016/j.rser.2025.116159},
    DOI = {10.1016/j.rser.2025.116159},
    journal = {Renewable and Sustainable Energy Reviews},
    publisher = {Elsevier BV},
    author = {Ji, Zhenya and Jiang, Ming},
    year = {2026},
    month = jan,
    pages = {116159},
}

@misc{lin2025understandingstragglerslargemodel,
    title = {Understanding Stragglers in Large Model Training Using What-if
             Analysis},
    author = {Jinkun Lin and Ziheng Jiang and Zuquan Song and Sida Zhao and
              Menghan Yu and Zhanghan Wang and Chenyuan Wang and Zuocheng Shi and
              Xiang Shi and Wei Jia and Zherui Liu and Shuguang Wang and Haibin
              Lin and Xin Liu and Aurojit Panda and Jinyang Li},
    year = {2025},
    eprint = {2505.05713},
    archivePrefix = {arXiv},
    primaryClass = {cs.DC},
    url = {https://arxiv.org/abs/2505.05713},
}

@misc{patterson2021carbonemissionslargeneural,
    title = {Carbon Emissions and Large Neural Network Training},
    author = {David Patterson and Joseph Gonzalez and Quoc Le and Chen Liang and
              Lluis-Miquel Munguia and Daniel Rothchild and David So and Maud
              Texier and Jeff Dean},
    year = {2021},
    eprint = {2104.10350},
    archivePrefix = {arXiv},
    primaryClass = {cs.LG},
    url = {https://arxiv.org/abs/2104.10350},
}

@inproceedings{xu2024efficient,
    author = {Xu, Wubiao and Huang, Xin and Meng, Shiman and Zhang, Weiping and
              Guo, Luanzheng and Sato, Kento},
    title = {An Efficient Checkpointing System for Large Machine Learning Model
             Training},
    booktitle = {Workshops of the International Conference on High Performance
                 Computing, Network, Storage, and Analysis (SC-W)},
    year = {2024},
    month = {11},
    pages = {896--900},
    doi = {10.1109/SCW63240.2024.00127},
}

@misc{zhang2025gockptgradientassistedmultistepoverlapped,
    title = {GoCkpt: Gradient-Assisted Multi-Step overlapped Checkpointing for
             Efficient LLM Training},
    author = {Keyao Zhang and Yiquan Chen and Zhuo Hu and Wenhai Lin and
              Jiexiong Xu and Wenzhi Chen},
    year = {2025},
    eprint = {2511.07035},
    archivePrefix = {arXiv},
    primaryClass = {cs.OS},
    url = {https://arxiv.org/abs/2511.07035},
}

@misc{kaplan2020scalinglawsneurallanguage,
    title = {Scaling Laws for Neural Language Models},
    author = {Jared Kaplan and Sam McCandlish and Tom Henighan and Tom B. Brown
              and Benjamin Chess and Rewon Child and Scott Gray and Alec Radford
              and Jeffrey Wu and Dario Amodei},
    year = {2020},
    eprint = {2001.08361},
    archivePrefix = {arXiv},
    primaryClass = {cs.LG},
    url = {https://arxiv.org/abs/2001.08361},
}

@inproceedings{9499454,
    author = {Mastenbroek, Fabian and Andreadis, Georgios and Jounaid, Soufiane
              and Lai, Wenchen and Burley, Jacob and Bosch, Jaro and van Eyk,
              Erwin and Versluis, Laurens and van Beek, Vincent and Iosup,
              Alexandru},
    booktitle = {2021 IEEE/ACM 21st International Symposium on Cluster, Cloud
                 and Internet Computing (CCGrid)},
    title = {OpenDC 2.0: Convenient Modeling and Simulation of Emerging
             Technologies in Cloud Datacenters},
    year = {2021},
    volume = {},
    number = {},
    pages = {455-464},
    keywords = {Procurement;Cloud computing;Analytical models;Computational
                modeling;Machine learning;Reproducibility of
                results;Stakeholders;OpenDC;datacenter;simulation;modeling;usecases;experimentation;performance
                analysis},
    doi = {10.1109/CCGrid51090.2021.00055},
}

@misc{svedas2025surveyendtoendmodelingdistributed,
    title = {A Survey of End-to-End Modeling for Distributed DNN Training:
             Workloads, Simulators, and TCO},
    author = {Jonas Svedas and Hannah Watson and Nathan Laubeuf and Diksha
              Moolchandani and Abubakr Nada and Arjun Singh and Dwaipayan Biswas
              and James Myers and Debjyoti Bhattacharjee},
    year = {2025},
    eprint = {2506.09275},
    archivePrefix = {arXiv},
    primaryClass = {cs.DC},
    url = {https://arxiv.org/abs/2506.09275},
}

@article{lin2025stragglers,
    title = {Understanding Stragglers in Large Model Training Using What-if
             Analysis},
    author = {Lin, Jinkun and Jiang, Ziheng and Song, Zuquan and Zhao, Sida and
              Yu, Menghan and Wang, Zhanghan and Wang, Chenyuan and Shi, Zuocheng
              and Shi, Xiang and Jia, Wei and others},
    journal = {arXiv preprint arXiv:2505.05713},
    year = {2025},
}


@article{10.5120/ijca2025925323,
    author = { Krishnam Raju Narsepalle },
    title = { Energy-Efficient Training and Inference in Large Language Models:
             Optimizing Computational and Energy Costs },
    journal = { International Journal of Computer Applications },
    issue_date = { Jun 2025 },
    volume = { 187 },
    number = { 14 },
    month = { Jun },
    year = { 2025 },
    issn = { 0975-8887 },
    pages = { 1-13 },
    numpages = {9},
    url = {
           https://ijcaonline.org/archives/volume187/number14/energy-efficient-training-and-inference-in-large-language-models-optimizing-computational-and-energy-costs/
           },
    doi = { 10.5120/ijca2025925323 },
    publisher = {Foundation of Computer Science (FCS), NY, USA},
    address = {New York, USA},
}

@misc{liang2025lumosefficientperformancemodeling,
    title = {Lumos: Efficient Performance Modeling and Estimation for
             Large-scale LLM Training},
    author = {Mingyu Liang and Hiwot Tadese Kassa and Wenyin Fu and Brian
              Coutinho and Louis Feng and Christina Delimitrou},
    year = {2025},
    eprint = {2504.09307},
    archivePrefix = {arXiv},
    primaryClass = {cs.DC},
    url = {https://arxiv.org/abs/2504.09307},
}

@misc{feng2024echosimulatingdistributedtraining,
    title = {Echo: Simulating Distributed Training At Scale},
    author = {Yicheng Feng and Yuetao Chen and Kaiwen Chen and Jingzong Li and
              Tianyuan Wu and Peng Cheng and Chuan Wu and Wei Wang and Tsung-Yi
              Ho and Hong Xu},
    year = {2024},
    eprint = {2412.12487},
    archivePrefix = {arXiv},
    primaryClass = {cs.LG},
    url = {https://arxiv.org/abs/2412.12487},
}

    @inproceedings{Huang2025flowcheck,
    series = {EuroSys ’25},
    title = {FlowCheck: Decoupling Checkpointing and Training of Large-Scale
             Models},
    url = {http://dx.doi.org/10.1145/3689031.3696088},
    DOI = {10.1145/3689031.3696088},
    booktitle = {Proceedings of the Twentieth European Conference on Computer
                 Systems},
    publisher = {ACM},
    author = {Huang, Zimeng and Nie, Hao and Jia, Haonan and Jiang, Bo and Guo,
              Junchen and Lu, Jianyuan and Wen, Rong and Lyu, Biao and Zhu,
              Shunmin and Wang, Xinbing},
    year = {2025},
    month = mar,
    pages = {1334–1349},
    collection = {EuroSys ’25},
}


@misc{zhang2022optopenpretrainedtransformer,
    title = {OPT: Open Pre-trained Transformer Language Models},
    author = {Susan Zhang and Stephen Roller and Naman Goyal and Mikel Artetxe
              and Moya Chen and Shuohui Chen and Christopher Dewan and Mona Diab
              and Xian Li and Xi Victoria Lin and Todor Mihaylov and Myle Ott and
              Sam Shleifer and Kurt Shuster and Daniel Simig and Punit Singh
              Koura and Anjali Sridhar and Tianlu Wang and Luke Zettlemoyer},
    year = {2022},
    eprint = {2205.01068},
    archivePrefix = {arXiv},
    primaryClass = {cs.CL},
    url = {https://arxiv.org/abs/2205.01068},
}

@inproceedings{won2023astrasim,
    title = {ASTRA-sim2.0: Modeling Hierarchical Networks and Disaggregated
             Systems for Large-model Training at Scale},
    url = {http://dx.doi.org/10.1109/ISPASS57527.2023.00035},
    DOI = {10.1109/ispass57527.2023.00035},
    booktitle = {2023 IEEE International Symposium on Performance Analysis of
                 Systems and Software (ISPASS)},
    publisher = {IEEE},
    author = {Won, William and Heo, Taekyung and Rashidi, Saeed and Sridharan,
              Srinivas and Srinivasan, Sudarshan and Krishna, Tushar},
    year = {2023},
    month = apr,
    pages = {283–294},
}
