
\section{Introduction}
Artificial Intelligence (AI) has shifted from a niche research field to a key factor of global
datacenter demand. This AI era has introduced a new class of workloads, like the training of Large
Language Models (LLMs). These type of workloads are not short-lived inference tasks or traditional
web services, but they are synchronous, monolithic, and resource-intensive training jobs that
occupies thousands of GPUs for months at a time \cite{jiang2024megascalescalinglargelanguage,
narayanan2021efficientlargescalelanguagemodel} . Estimating the carbon footprint of models like
BLOOM reveals that training alone can emit over 50 tonnes of
CO2\cite{luccioni2022estimatingcarbonfootprintbloom}. Efficiency in these systems is not just a
performance metric, but a necessity for both financial and environmental sustainability. \medskip
\\ Training is estimated to account for 10-40\% of the total energy consumed by an LLM in its
lifetime\cite{Ji2026-gp}. This is substantial and optimizing this to reduce even a few percent
would cut costs and emissions. Discrete-event simulators could get every bit of performance out of
a system resulting into millions of dollars in operational savings and significant reduction in
carbon emissions. These simulators Act as a critical lever in this optimization, enabling companies
to identify these efficiency gains without the capital risk of physical experimentation.\medskip \\
Datacenter configurations where all compute nodes (GPUs) are identical, an example being 1000
NVIDIA H100s, is the current industrial standard for training Frontier Models to minimize the
"straggler effect" and synchronization latency found in heterogeneous environments. Even in these
heterogeneous clusters, there is still a lot of time lost on synchronization. A simple background
process can stall thousands of others during gradient
synchronization\cite{lin2025understandingstragglerslargemodel}. Optimizing LLM training is
notoriously difficult because physical experimentation is expensive. Operators often over-provision
hardware to ensure stability, avoiding the risk of potential downtime but leading to massive
resource waste\cite{patterson2021carbonemissionslargeneural}. Current simulators also fail to model
the "system-level" components of training, specifically Checkpointing mechanisms (saving model
states to disk in case of failure) and gradient synchronization (network communication between
clusters). Recent surveys insidate that 30-40\% of training time can be lost due to these overheads
if not optimized, but standard simulators still treat them as negligible\cite{xu2024efficient,
zhang2025gockptgradientassistedmultistepoverlapped}.\medskip \\ Key prior work relies on the
"Scaling Laws" of Neural Networks\cite{kaplan2020scalinglawsneurallanguage}, which which predict
the raw compute requirements for training models based on their size. To meet these demands,
industry has adopted distributed training systems like
Megatron-LM\cite{narayanan2021efficientlargescalelanguagemodel}, which split the workload across
thousands of GPUs. In the field of simulation, tools like OpenDC\cite{9499454} and ASTRA-sim have
successfully modeled general cloud and network behaviors. Still, a gap remains: recent
surveys\cite{dryden2025survey} and production traces\cite{lin2025stragglers} show that current
simulators fail to account for the "system-level" friction of training, specifically the massive
overheads of checkpointing and synchronization. This is a critical gap, as existing tools cannot
accurately predict the true energy and financial impact of large-scale training workloads.\medskip
\\So this leads us to the main research question: \textbf{How can we utilize discrete-event
simulation to determine the cost-optimal and energy-optimal homogeneous cluster configuration for
training Large Language Models?} This will become the first open-source LLM Training Workload Model
for OpenDC that specifically accounts for reliability overheads like checkpointing and gradient
synchronization. Allowing researchers to study "Green AI" infrastructure without needing access to
supercomputers to do these experiments. This project will contribute to the scientific community by
providing a new tool for simulating LLM training, enabling more efficient and sustainable AI
development. It will also help me develop my skills in simulation, distributed systems, and AI,
preparing me for a career in this rapidly evolving field.
