
\section{Problem}
We identified a critical gap in the current simulation tools available: \textbf{There is no
accessible instrument to accurately predict the cost-efficiency and energy impact of LLM training
configurations on homogeneous clusters}. While there is a growing body of research that has focused
on Model Optimization, reducing computational complexity by up to 35\% through techniques like
sparse training and quantization\cite{10.5120/ijca2025925323}, far less attention has been given to
System Optimization. Even with highly compressed models, the infrastructure overheads of fault
tolerance and synchronization persist. Current simulators often focus on the former (compute time),
ignoring the latter (system-level overheads), creating a blind spot in energy estimation.
Specifically, we identify the following issues:
\begin{itemize}
	\item \textbf{The Fallacy of Idealized Execution}: Existing simulators normally model training
	      under the assumption of ideal execution, treating homogeneous clusters as perfectly
	      synchronized straggler free systems. This ignores the non-deterministic delays of 3D
	      parallelism. As revealed from production traces, even in homogeneous clusters designed
	      for stability, "software stragglers" (caused by uneven pipeline partitioning or garbage
	      collection pauses) can cause the system to waste up to 45\% of its allocated GPU
	      resources\cite{lin2025stragglers}. Current tools are unable to capture these synchronization inefficiencies, resulting in
	      overly optimistic performance and energy estimates that do not reflect real-world efficiency
	      losses.
	\item \textbf{Ignored Reliability Costs}: Fault tolerance is mandatory for the month-long duration of training
	      large LLMs on thousands of GPUs. However, the energy and time cost of Checkpointing is rarely modeled in
	      planning tools. Data shows that standard asynchronous checkpointing can stall GPU computation for over
	      1.3 seconds per save interval\cite{zhang2025gockptgradientassistedmultistepoverlapped}.
	      In a large-scale cluster with frequent checkpoints, these delays accumulate, leading to a significant
	      and unoptimized energy overhead that stays regardless of how optimized the model architecture itself is.
	\item \textbf{The Absence of Multi-Objective Trade-off Analysis}: Due to it being almost impossible to predict these
	      system-level overheads, operators are forced to rely on massive over-provisioning or guessing.
	      There is a lack of research quantifying the trade-off between speed, money and carbon emissions.
	      For example, slowing down training slightly to accommodate a more energy-efficient checkpointing
	      schedule could save megawatt-hours of energy, but without a simulator to quantify this trade-off,
	      these optimizations are considered too risky to attempt on live production hardware.
\end{itemize}
