\section{Background}

\subsection{The lifecycle of Large Language Models: Training vs Inference}
It is crucial to distinguish between the two primary phases of an LLM's lifecycle: Training and
Inference.\\ \textbf{Inference} is the process of generating a response from a trained model. It is
stateless, highly parallelizable, and typically consumes a negligible amount of energy per request.
Text classification for example consumes only 0.002Wh per request\cite{Ji2026-gp}.\\
\textbf{Training} is the focus of this research, involving a stateful, monolithic workload that is
iterating billions of parameters across thousands of GPUs. While inference accounts for the
majority of accumulated energy usage over years of deployment, training is the most capital and
resource intensive phase per unit of time. Training for GPT-4 is estimated to have an energy
consumption of 51,772 to 62,318 MWh\cite{Ji2026-gp}. This is the equivalent to around 5,000 to
6,000 average American households' annual electricity consumption. Optimization of this phase is
critical because unlike inference, which can be distributed across edge devices, training requires
a concentrated, high-performance datacenter environment where inefficiencies scale with cluster
size.
\subsection{Homogeneous Cluster and 3D Parallelism}
To manage the computational scale of LLM training, the industry has standardized on 3D parallelism.
This is a hybrid strategy that combines Data, Tensor and Pipeline parallelism to partition models
across thousands of GPUs\cite{narayanan2021efficientlargescalelanguagemodel}. To support this
synchronization, operators almost exclusively deploy Homogeneous Clusters, where all the compute
nodes are identical. The "straggler effect," in which slower nodes cause the entire training task
to be delayed because quicker nodes must wait for slower nodes to complete, is minimized by
enforcing homogeneity\cite{lin2025stragglers}. However, even in these homogeneous environments,
recent studies using production traces have shown that hardware homogeneity does not guarantee
performance homogeneity. Software-level inefficiencies, such as uneven pipeline partitioning or
pauses from garbage collection, can still cause a significant synchronization delay across nodes.
These "software stragglers" can cause up to 45\% of allocated GPU resources to be wasted, even in
clusters with identical hardware specification\cite{lin2025stragglers}.
\subsection{Checkpointing and Fault Tolerance}
Training runs take a very long time, often multiple weeks or months. In this period, hardware failures
are statistically bound to happen. To make sure fault tolerance is effective, training systems
implement checkpointing: The process of periodically pausing the computation to save the model's
parameters and optimizer states to persistent storage (SSDs). Although this is necessary, this
procedure introduces a significant Checkpointing Overhead. For every save operation, standard
asynchronous checkpointing can stop training for more than 1.3 seconds
\cite{zhang2025gockptgradientassistedmultistepoverlapped}. In a large-scale cluster where saving
occurs frequently to mitigate failure risks, these stalls represent a massive efficiency loss.
Techniques like "Gradient-Assisted Checkpointing" attempt to reduce this stall time to sub-second
levels\cite{zhang2025gockptgradientassistedmultistepoverlapped}, but their impact on total energy
consumption at the scale of 10,000+ GPUs has not yet been simulated.
\subsection{Discrete-Event Simulation (OpenDC)}
The problem with analyzing these inefficiencies on real hardware is the cost. One cannot simply
experiment with different configurations on a 10,000 GPU cluster or simply "pause" a \$50 million
training run to test a new scheduling hypothesis. Discrete-Event Simulation (DES) offers a solution
by modeling the system as a sequence of events in time. OpenDC\cite{9499454} is a state-of-the-art
DES platform that has successfully modeled cloud datacenter workloads and serverless functions.
However, current simulators often treat jobs as generic units of compute duration and assume that
small-scale node behavior is still correct when scaled up\cite{dryden2025survey}. They lack the
specific workload models required to simulate the internal dynamics of LLM training, specifically
the network bursts of 3D parallelism and the I/O stalls of checkpointing, rendering them
insufficient for precise capacity planning in the context of LLM training.
